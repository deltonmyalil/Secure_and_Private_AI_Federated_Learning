{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FL_Course_part_14_Secure_Federated_Learning_with_Trusted_Aggregator.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM9MwcnmiGjzZrgvAvHOOoS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l427Q8NmAjbf",
        "outputId": "0ce09905-8d7c-4374-bbc5-56a66dc80638"
      },
      "source": [
        "!pip install syft==0.2.9 >/dev/null"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement notebook~=5.3.0; python_version >= \"3.0\", but you'll have notebook 5.7.8 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=5.1.0; python_version >= \"3.0\", but you'll have tornado 4.5.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: bokeh 2.1.1 has requirement tornado>=5.1, but you'll have tornado 4.5.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l-zQyb5EdVO"
      },
      "source": [
        "Models are updated to a trusted third party - let's call it secureWorker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knG71aPVElo5"
      },
      "source": [
        "import syft\r\n",
        "import torch\r\n",
        "from torch import nn, optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87qt2P_oEs4K"
      },
      "source": [
        "hook = syft.TorchHook(torch)\r\n",
        "\r\n",
        "# Creating two workers\r\n",
        "bob = syft.VirtualWorker(hook, id='bob')\r\n",
        "alice = syft.VirtualWorker(hook, id='alice')\r\n",
        "# Creating secure worker\r\n",
        "secure_worker = syft.VirtualWorker(hook, id='secure_worker')\r\n",
        "\r\n",
        "# Toy Dataset\r\n",
        "data = torch.tensor([[0,0], [0,1], [1,0], [1., 1]], requires_grad=True)\r\n",
        "target = torch.tensor([[0],[0],[1],[1.]], requires_grad=True)\r\n",
        "\r\n",
        "# Get pointers to data on each worker by sending data to bob and alice\r\n",
        "bobs_data = data[0:2].send(bob)\r\n",
        "bobs_target = target[0:2].send(bob)\r\n",
        "\r\n",
        "alices_data = data[2:].send(alice)\r\n",
        "alices_target = target[2:].send(alice)\r\n",
        "\r\n",
        "# Simple Linear Model\r\n",
        "model = nn.Linear(2, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwuEZDs6L7Dk"
      },
      "source": [
        "# Sending model's copies to bob and alice\r\n",
        "bobs_model = model.copy().send(bob)\r\n",
        "alices_model = model.copy().send(alice)\r\n",
        "\r\n",
        "# Two different models mean two different optimizers\r\n",
        "bobs_opt = optim.SGD(params=bobs_model.parameters(), lr=0.1)\r\n",
        "alices_opt = optim.SGD(params=alices_model.parameters(), lr=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYdvYopGMhwX",
        "outputId": "acb4b330-f5b0-4f45-d7c7-3a1e19b12603"
      },
      "source": [
        "# A single step of training for bob is like this\r\n",
        "\r\n",
        "# Zeroing out gradients\r\n",
        "bobs_opt.zero_grad()\r\n",
        "# Predicting, finding loss and backpropagate\r\n",
        "bobs_pred = bobs_model(bobs_data)\r\n",
        "bobs_loss = ((bobs_pred-bobs_target)**2).sum()\r\n",
        "bobs_loss.backward()\r\n",
        "# Updating weights\r\n",
        "bobs_opt.step()\r\n",
        "# Get bob's loss\r\n",
        "bobs_loss = bobs_loss.get().data\r\n",
        "bobs_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.3651)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm4RlidhNLwL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e58920e2-cfae-4984-cd78-a94f9315e744"
      },
      "source": [
        "# Same procedure follows for alice as \r\n",
        "alices_opt.zero_grad()\r\n",
        "alices_pred = alices_model(alices_data)\r\n",
        "alices_loss = ((alices_pred-alices_target)**2).sum()\r\n",
        "alices_loss.backward()\r\n",
        "alices_opt.step()\r\n",
        "alices_loss = alices_loss.get().data\r\n",
        "alices_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.8685)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6cSll87Np0n",
        "outputId": "87e60ec8-bb53-4fd4-b0cb-089e32164549"
      },
      "source": [
        "# Put the above two in loop to train them for a few rounds\r\n",
        "\r\n",
        "epochs = 20\r\n",
        "for i in range(epochs):\r\n",
        "\r\n",
        "    # For Bob\r\n",
        "    bobs_opt.zero_grad()\r\n",
        "    bobs_pred = bobs_model(bobs_data)\r\n",
        "    bobs_loss = ((bobs_pred-bobs_target)**2).sum()\r\n",
        "    bobs_loss.backward()\r\n",
        "    bobs_opt.step()\r\n",
        "    bobs_loss = bobs_loss.get().data\r\n",
        "    print('Bobs Loss:', bobs_loss)\r\n",
        "    # For Alice\r\n",
        "    alices_opt.zero_grad()\r\n",
        "    alices_pred = alices_model(alices_data)\r\n",
        "    alices_loss = ((alices_pred-alices_target)**2).sum()\r\n",
        "    alices_loss.backward()\r\n",
        "    alices_opt.step()\r\n",
        "    alices_loss = alices_loss.get().data\r\n",
        "    print('Alices loss:',alices_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bobs Loss: tensor(0.3120)\n",
            "Alices loss: tensor(0.1593)\n",
            "Bobs Loss: tensor(0.0727)\n",
            "Alices loss: tensor(0.1282)\n",
            "Bobs Loss: tensor(0.0181)\n",
            "Alices loss: tensor(0.1067)\n",
            "Bobs Loss: tensor(0.0055)\n",
            "Alices loss: tensor(0.0888)\n",
            "Bobs Loss: tensor(0.0024)\n",
            "Alices loss: tensor(0.0739)\n",
            "Bobs Loss: tensor(0.0015)\n",
            "Alices loss: tensor(0.0615)\n",
            "Bobs Loss: tensor(0.0012)\n",
            "Alices loss: tensor(0.0512)\n",
            "Bobs Loss: tensor(0.0010)\n",
            "Alices loss: tensor(0.0426)\n",
            "Bobs Loss: tensor(0.0008)\n",
            "Alices loss: tensor(0.0355)\n",
            "Bobs Loss: tensor(0.0007)\n",
            "Alices loss: tensor(0.0295)\n",
            "Bobs Loss: tensor(0.0006)\n",
            "Alices loss: tensor(0.0246)\n",
            "Bobs Loss: tensor(0.0005)\n",
            "Alices loss: tensor(0.0204)\n",
            "Bobs Loss: tensor(0.0004)\n",
            "Alices loss: tensor(0.0170)\n",
            "Bobs Loss: tensor(0.0004)\n",
            "Alices loss: tensor(0.0142)\n",
            "Bobs Loss: tensor(0.0003)\n",
            "Alices loss: tensor(0.0118)\n",
            "Bobs Loss: tensor(0.0003)\n",
            "Alices loss: tensor(0.0098)\n",
            "Bobs Loss: tensor(0.0002)\n",
            "Alices loss: tensor(0.0082)\n",
            "Bobs Loss: tensor(0.0002)\n",
            "Alices loss: tensor(0.0068)\n",
            "Bobs Loss: tensor(0.0002)\n",
            "Alices loss: tensor(0.0057)\n",
            "Bobs Loss: tensor(0.0001)\n",
            "Alices loss: tensor(0.0047)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gti7OcP7Ocf1"
      },
      "source": [
        "Now we have a trained model for each bob and alice. how to aggregate them?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAOEJZ84OkLG",
        "outputId": "09e6f3e5-474e-4e93-c145-b2a5a58fa566"
      },
      "source": [
        "alices_model.move(secure_worker)\r\n",
        "bobs_model.move(secure_worker)\r\n",
        "secure_worker._objects"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{20907566982: Parameter containing:\n",
              " tensor([0.9622], requires_grad=True), 50686509638: Parameter containing:\n",
              " tensor([[-0.4107,  0.0154]], requires_grad=True), 59568901628: Parameter containing:\n",
              " tensor([-0.0095], requires_grad=True), 95708304698: Parameter containing:\n",
              " tensor([[-0.0115,  0.0879]], requires_grad=True)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Garw3a-UOsmz"
      },
      "source": [
        "# Simple averaging on secure_worker\r\n",
        "# here model is the global model. Alice's Model and Bob's model resides in secure_worker\r\n",
        "# model resides in 'me'. Secure worker averages it and then sends it to 'me'.\r\n",
        "'''\r\n",
        "model.weight.data.set_() might not work for v0.2.9.\r\n",
        "So change it as model.weight.set_().\r\n",
        "But this will create race condition/error with autograd so,\r\n",
        "use in scope of torch.no_grad()\r\n",
        "'''\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "    model.weight.set_(((alices_model.weight.data + bobs_model.weight.data) / 2).get())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9VWHDTWQ6k4",
        "outputId": "c6bcce76-8a2b-490a-c9cd-cf147421a699"
      },
      "source": [
        "model.weight"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.2111,  0.0516]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfgMo1J_RaDH"
      },
      "source": [
        "with torch.no_grad():\r\n",
        "    model.bias.set_(((alices_model.bias.data + bobs_model.bias.data) / 2).get())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fiqpwycS2-7",
        "outputId": "1c1f3861-6af4-4acf-bdc9-e0d4e1a350da"
      },
      "source": [
        "model.bias"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([0.4763], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8Mnk4ohS4hv",
        "outputId": "cc4ec70e-4f77-4908-dfef-b1b4b39b6af1"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=2, out_features=1, bias=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOFlh7QYS6uW"
      },
      "source": [
        "# This is the central model\r\n",
        "model.location # Does not return anything... 'me' would have been nice, as anyway pytorch is overriden by syft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2TPjs4STXDK"
      },
      "source": [
        "This is the basic trusted aggregation workflow. \r\n",
        "* Create a secure worker object from VirtualWorker()\r\n",
        "* Move the clients' models to secure worker\r\n",
        "* Average the weights and biases (implicitly on the secure worker) and then take just the result to model (Global model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BVnBxIQUPhu",
        "outputId": "8277b484-0cf7-42f4-a392-bf6991cd7681"
      },
      "source": [
        "# Put this in a loop\r\n",
        "federated_rounds = 10\r\n",
        "epochs = 20\r\n",
        "lr = 0.1\r\n",
        "\r\n",
        "for federated_round_number in range(federated_rounds):\r\n",
        "\r\n",
        "    bobs_model = model.copy().send(bob)\r\n",
        "    alices_model = model.copy().send(alice)\r\n",
        "\r\n",
        "    bobs_opt = optim.SGD(params=bobs_model.parameters(), lr=lr)\r\n",
        "    alices_opt = optim.SGD(params=alices_model.parameters(), lr=lr)\r\n",
        "\r\n",
        "    for epoch in range(epochs):\r\n",
        "\r\n",
        "        bobs_opt.zero_grad()\r\n",
        "        bobs_pred = bobs_model(bobs_data)\r\n",
        "        bobs_loss = ((bobs_pred - bobs_target)**2).sum()\r\n",
        "        bobs_loss.backward()\r\n",
        "        bobs_opt.step()\r\n",
        "        bobs_loss = bobs_loss.get().data\r\n",
        "\r\n",
        "        alices_opt.zero_grad()\r\n",
        "        alices_pred = alices_model(alices_data)\r\n",
        "        alices_loss = ((alices_pred - alices_target)**2).sum()\r\n",
        "        alices_loss.backward()\r\n",
        "        alices_opt.step()\r\n",
        "        alices_loss = alices_loss.get().data\r\n",
        "\r\n",
        "    bobs_model.move(secure_worker)\r\n",
        "    alices_model.move(secure_worker)\r\n",
        "\r\n",
        "    # Secure Aggregation Logic\r\n",
        "    with torch.no_grad():\r\n",
        "        model.weight.set_(((alices_model.weight.data + bobs_model.weight.data) / 2).get())\r\n",
        "        model.bias.set_(((alices_model.bias.data + bobs_model.bias.data) / 2).get())\r\n",
        "    \r\n",
        "    print('At Federated Round {}, Bob\\'s Loss = {}; Alice\\'s Loss = {}'.format(federated_round_number, bobs_loss, alices_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "At Federated Round 0, Bob's Loss = 0.0011702917981892824; Alice's Loss = 0.0002818136417772621\n",
            "At Federated Round 1, Bob's Loss = 0.0008468828164041042; Alice's Loss = 0.00020137940009590238\n",
            "At Federated Round 2, Bob's Loss = 0.000613740470726043; Alice's Loss = 0.00015224932576529682\n",
            "At Federated Round 3, Bob's Loss = 0.00045033235801383853; Alice's Loss = 0.00011353669833624735\n",
            "At Federated Round 4, Bob's Loss = 0.00033197057200595737; Alice's Loss = 8.410410373471677e-05\n",
            "At Federated Round 5, Bob's Loss = 0.00024505818146280944; Alice's Loss = 6.216921610757709e-05\n",
            "At Federated Round 6, Bob's Loss = 0.00018097036809194833; Alice's Loss = 4.592759069055319e-05\n",
            "At Federated Round 7, Bob's Loss = 0.00013365710037760437; Alice's Loss = 3.3924199669854715e-05\n",
            "At Federated Round 8, Bob's Loss = 9.871630754787475e-05; Alice's Loss = 2.5056135200429708e-05\n",
            "At Federated Round 9, Bob's Loss = 7.291034853551537e-05; Alice's Loss = 1.850617627496831e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN2EKRykYW_3"
      },
      "source": [
        "Here the models are sent to a third party secure aggregator. It performs the (alice+bob)/2 there itself and will give you the model parameters when you call get() on the result.\r\n",
        "\r\n",
        "Hence, the root server ('me') does not actually see alice's and bob's model, it only gets an aggregation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iClmARlGZVXs"
      },
      "source": [
        "Shortcomings:\r\n",
        "1. Aggregator requires trust\r\n",
        "2. Will fail in Curious Aggregator Threat model.\r\n",
        "3. Here, in this code, model training at bob and alice is sequential. On different physical machines, do it in paralel. \r\n",
        "4. Asynchronous model training and aggregation will be better for cross device federated learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsukD-rRZlwK"
      },
      "source": [
        "Fix:\r\n",
        "\r\n",
        "Use Secure Multiparty Computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkRGOU_EakWW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}